{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Collecting Textual Data from Web: Web Scraping\n",
    "=====\n",
    "Computational Text Analysis\n",
    "\n",
    "Bao Yang, ACEM, SJTU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Announcements\n",
    "---\n",
    "* Lab 1 assignment is due today!\n",
    "* Some of you may not be able to install jupyter correctly because of a bug of the latest version of miniconda3: https://github.com/ContinuumIO/anaconda-issues/issues/1424. Two solutions:\n",
    "    - install a previous version (4.2.12) of miniconda3: https://repo.continuum.io/miniconda/\n",
    "    - install anaconda3 which includes jupyter by default: https://www.continuum.io/downloads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Contents\n",
    "===\n",
    "* Introduction\n",
    "    - **What is web scraping? and Why?**\n",
    "    - Robots.txt：爬虫/机器人协议\n",
    "* Crawling Webpage\n",
    "    - urllib in standard library\n",
    "    - Handling encoding and exception issues\n",
    "    - requests: HTTP for Humans\n",
    "* Parsing Webpage\n",
    "    - HTML Basics\n",
    "    - BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is web scraping?\n",
    "===\n",
    "* Obtaining data from webpages\n",
    "* Two methods:\n",
    "    - web scraping: crawl and parse HTML webpages\n",
    "    - using API (应用程序接口)\n",
    "        + convenient and well-structured, but not provided by all websites\n",
    "        + e.g., Python财经数据接口包: http://tushare.org/ \n",
    "        + installation: pip install tushare, pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Demo: tushare\n",
    "import tushare as ts # import first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df = ts.get_hist_data('000002') #获取三年内全部日k线数据\n",
    "df.head(10) # pandas的DataFrame数据结构，打印前10条记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df = ts.get_today_all() # 获取当日交易所有股票的行情数据\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df = ts.get_tick_data('000002',date='2017-03-01') # 获取历史分笔数据明细\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ts.get_realtime_quotes(['000002','600115','600221']) # 获取实时分笔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df = ts.get_index() # 获取大盘指数行情\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df = ts.get_sina_dd('000002', date='2017-03-01', vol=500)  #获取大笔交易数据（指定大于等于500手）\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df = ts.forecast_data(2017,1) # 获取2017年第1季度的业绩预告\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df = ts.new_stocks() # 获取新股数据\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df = ts.get_stock_basics() # 获取基本面数据\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ts.get_latest_news(top=5,show_content=True) #显示最新5条新闻，并打印出新闻内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df = ts.realtime_boxoffice() # 获取实时电影票房数据，30分钟更新一次票房数据\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df = ts.month_boxoffice('2017-01') #获取2017年1月票房数据\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**More data in TuShare to explore: http://tushare.org/**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What is web scraping?\n",
    "===\n",
    "* How to scrape web data?\n",
    "    - crawl webpages: urllib, requests, etc.\n",
    "    - parse webpages: BeautifulSoup, re (regular expression), etc.\n",
    "    - more sophisicated framework for large-scale crawling: Scrapy, search engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Why web scraping?\n",
    "===\n",
    "* Tremendous amount of useful information\n",
    "    - online reviews (e-commerce, healthcare, etc.)\n",
    "    - financial news in traditional media and social media\n",
    "    - corporate disclosures (annual reports, IPO prospectus, etc.)\n",
    "    - product information (air ticket price, e-commerce, etc.)\n",
    "    - any more examples in your research field?\n",
    "        + start to brainstorm for your course project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Why web scraping?\n",
    "===\n",
    "* Infeasible to collect by hand, need to automate the collection\n",
    "* It is fun, and allows you to do something really useful and cool!\n",
    "    - Any thoughts/example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Web scraping is about obtaining data from webpages. There is low level scraping where you parse the data out of the html code of the webpage. There also is scraping over APIs from websites who try to make your life a bit easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Read and Tweet!\n",
    "=================\n",
    "\n",
    "![ReadTweet](images/readtweet.jpg)\n",
    "\n",
    "* by Justin Blinder\n",
    "* http://projects.justinblinder.com/We-Read-We-Tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "“We Read, We Tweet” geographically visualizes the dissemination of New York Times articles through Twitter. Each line connects the location of a tweet to the contextual location of the New York Times article it referenced. The lines are generated in a sequence based on the time in which a tweet occurs. The project explores digital news distribution in a temporal and spatial context through the social space of Twitter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Twitter Sentiments\n",
    "===\n",
    "\n",
    "![TwitterSentiments](images/tweet-viz-ex.png \"Twitter Sentiments\")\n",
    "\n",
    "* by Healey and Ramaswamy\n",
    "* http://www.csc.ncsu.edu/faculty/healey/tweet_viz/tweet_app/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Type a keyword into the input field, then click the Query button. Recent tweets that contain your keyword are pulled from Twitter and visualized in the Sentiment tab as circles. Hover your mouse over a tweet or click on it to see its text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Contents\n",
    "===\n",
    "* Introduction\n",
    "    - What is web scraping? and Why?\n",
    "    - **Robots.txt：爬虫/机器人协议**\n",
    "* Crawling Webpage\n",
    "    - urllib in standard library\n",
    "    - Handling encoding and exception issues\n",
    "    - requests: HTTP for Humans\n",
    "* Parsing Webpage\n",
    "    - HTML Basics\n",
    "    - BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Robots.txt\n",
    "===\n",
    "* robots exclusion protocol (网络爬虫排除协议)\n",
    "    - https://en.wikipedia.org/wiki/Robots_exclusion_standard\n",
    "* gives instructions to web robots (e.g., search engines, your own crawler)\n",
    "    - what information can/cannot be scraped\n",
    "* specified by web site owner\n",
    "* is located at the top-level directory of the web server\n",
    "\n",
    "![Robots.txt](images/robots_txt.jpg \"Robots.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Robots.txt: an example\n",
    "---\n",
    "\n",
    "*** What does this one do? ***\n",
    "\n",
    "* https://www.taobao.com/robots.txt\n",
    "    - User-agent:  Baiduspider\n",
    "    - Allow:  /article\n",
    "    - Allow:  /oshtml\n",
    "    - Allow:  /wenzhang\n",
    "    - Disallow:  /product/\n",
    "    - Disallow:  /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Robots.txt in real-world\n",
    "---\n",
    "\n",
    "Please try to explore robots.txt in the following websites:\n",
    "* https://www.baidu.com\n",
    "* https://www.jd.com\n",
    "* https://www.amazon.cn\n",
    "* https://www.dianping.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Robots协议的遵守:\n",
    "---\n",
    "\n",
    "* Robots协议只是建议性的，不具有强制约束性\n",
    "* 大量爬取数据时建议遵守，否则可能存在法律风险\n",
    "* Robots协议潜在的安全风险？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Contents\n",
    "===\n",
    "* Introduction\n",
    "    - What is web scraping? and Why?\n",
    "    - Robots.txt：爬虫/机器人协议\n",
    "* Crawling Webpage\n",
    "    - **urllib in standard library**\n",
    "    - Handling encoding and exception issues\n",
    "    - requests: HTTP for Humans\n",
    "* Parsing Webpage\n",
    "    - HTML Basics\n",
    "    - BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "urllib package\n",
    "===\n",
    "\n",
    "* Python3 provides the **urllib** package in its standard libraries for opening and reading url links\n",
    "* Documentation: https://docs.python.org/3/library/urllib.html\n",
    "* Differences between Python2 and Python3: \n",
    "    - Python2 provides urllib2 package\n",
    "    - In Python3, urllib2 is splited into two parts\n",
    "        + urllib.request, and urllib.error\n",
    "* We mainly use urllib.request\n",
    "    - https://docs.python.org/3/library/urllib.request.html#module-urllib.request\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Demo: use urllib.request to open and read url links\n",
    "import urllib.request \n",
    "\n",
    "url = \"https://www.crummy.com/software/BeautifulSoup/\"\n",
    "response = urllib.request.urlopen(url)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(type(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "html = response.read()\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Demo: encoding issue\n",
    "import urllib.request\n",
    "\n",
    "url = \"https://www.douban.com\"\n",
    "response = urllib.request.urlopen(url)\n",
    "html = response.read()\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "html = html.decode('utf-8') # use decode() method to convert bytes to str\n",
    "print(type(html))\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Demo: encoding issue\n",
    "import urllib.request \n",
    "\n",
    "url = \"https://bbs.sjtu.edu.cn/php/bbsindex.html\"\n",
    "response = urllib.request.urlopen(url)\n",
    "html = response.read()\n",
    "html = html.decode('utf-8') # how to find the encoding of the webpage?\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Demo: encoding issue\n",
    "import urllib.request \n",
    "\n",
    "url = \"https://bbs.sjtu.edu.cn/php/bbsindex.html\"\n",
    "response = urllib.request.urlopen(url)\n",
    "html = response.read()\n",
    "html = html.decode('gb2312') # how to find the encoding of the webpage?\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Exercise:\n",
    "---\n",
    "* 使用urllib连接并读取url链接: https://www.crummy.com/software/BeautifulSoup/\n",
    "* 单词'Alice' 有没有在网页中出现?\n",
    "* 单词'Soup' 在网页中出现了多少次? (Hint: use .count() method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Contents\n",
    "===\n",
    "* Introduction\n",
    "    - What is web scraping? and Why?\n",
    "    - Robots.txt：爬虫/机器人协议\n",
    "* Crawling Webpage\n",
    "    - urllib in standard library\n",
    "    - **Handling encoding and exception issues**\n",
    "    - requests: HTTP for Humans\n",
    "* Parsing Webpage\n",
    "    - HTML Basics\n",
    "    - BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Encoding](images/encoding.png)\n",
    "* 字符编码是Python编程中常碰到的一个麻烦，相比于Python2，Python3已经对字符编码做了很大的优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "字符编码\n",
    "---\n",
    "* unicode\n",
    "    - python3中，所有的字符串在**内存**中均是用unicode编码保存\n",
    "    - 把所有语言统一到一套编码中，避免出现乱码\n",
    "    - 问题：浪费存储空间（英文字母也要使用多个字节表示）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* 可变长编码\n",
    "    - 在文件中存储字符时，将unicode转换成可变长编码，可以节省存储空间\n",
    "    - utf-8\n",
    "    - gb2312, gbk, gb18030 (子集关系，前者是后者的子集，后者可以兼容前者)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Python中字符编码的转换\n",
    "---\n",
    "* encode()方法：unicode编码转换成指定编码方式\n",
    "    - .encode('utf-8'), .encode('gb2312'), .encode('gbk'), .encode('gb18030')\n",
    "* decode()方法：指定编码方式转换为unicode编码\n",
    "    - .decode('utf-8'), .decode('gb2312'), .decode('gbk'), .decode('gb18030')\n",
    "* 在Python中，unicode是中间编码\n",
    "    - 编码A转换成编码B：string.decode(\"A\").encode(\"B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Demo: Bytes (utf-8, gb2312, gbk, gb18030) --> Str (Unicode)\n",
    "import urllib.request\n",
    "\n",
    "url = \"http://bbs.sjtu.edu.cn/php/bbsindex.html\"\n",
    "response = urllib.request.urlopen(url)\n",
    "html = response.read()\n",
    "print(type(html))\n",
    "html = html.decode('gb2312') # can we use gbk and gb18030 here?\n",
    "print(type(html))\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "注意：有时HTML网页头部中自己申明的字符编码方式有可能是错误的！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How to identify encoding automatically?\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.crummy.com/software/BeautifulSoup {'encoding': 'ascii', 'confidence': 1.0} ascii\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.douban.com {'encoding': 'utf-8', 'confidence': 0.99} utf-8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://bbs.sjtu.edu.cn/php/bbsindex.html {'encoding': 'GB2312', 'confidence': 0.99} GB2312\n"
     ]
    }
   ],
   "source": [
    "import chardet #自动检测编码, 安装：pip install chardet\n",
    "import urllib.request\n",
    "\n",
    "for url in [\"http://www.crummy.com/software/BeautifulSoup\",\n",
    "            \"https://www.douban.com\",\n",
    "            \"https://bbs.sjtu.edu.cn/php/bbsindex.html\"]:\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    mychar = chardet.detect(html)\n",
    "    print(url,mychar,mychar['encoding'])\n",
    "    # encoding = mychar['encoding']\n",
    "    # print(html.decode(encoding)[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Demo: automatically detect encoding\n",
    "import chardet\n",
    "import urllib.request\n",
    "\n",
    "url = \"https://www.douban.com\"\n",
    "html = urllib.request.urlopen(url).read()\n",
    "mychar = chardet.detect(html)\n",
    "print(mychar)\n",
    "print(html.decode(mychar['encoding'])) #自动解码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Handling Connection Exceptions\n",
    "===\n",
    "* 爬虫程序需要网络连接，但是网络可能存在风险\n",
    "* 爬虫程序需要处理网络连接异常，否则程序会崩溃"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* URLError：\n",
    "    - 可能的原因：没连上网, 连接不到服务器, 服务器不存在 ...\n",
    "* HTTPError（URLError的子类），会产生一个HTTP状态码，例如：\n",
    "    - 400非法请求，401未授权，403禁止，404：没有找到网页 ...\n",
    "    - more status codes: https://en.wikipedia.org/wiki/List_of_HTTP_status_codes       \n",
    "* 我们可以用try-except语句来捕获并处理相应的异常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Demo: URLError\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "\n",
    "try:\n",
    "    urllib.request.urlopen(\"https://www.youtube.com\")\n",
    "except urllib.error.URLError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Demo: HTTPError\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "\n",
    "try:\n",
    "    urllib.request.urlopen(\"http://blog.csdn.net/cqcre\")\n",
    "#     urllib.request.urlopen(\"http://www.sjtu.edu.cn/1234\")\n",
    "#     urllib.request.urlopen(\"http://www.baidu.com\")\n",
    "except urllib.error.HTTPError as e:\n",
    "    print(e)\n",
    "except urllib.error.URLError as e:\n",
    "    print(e)\n",
    "else:\n",
    "    print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "将爬虫伪装成浏览器\n",
    "---\n",
    "\n",
    "* 有时http错误(e.g., 403)可能是由于网站禁止爬虫\n",
    "* 我们可以加上一些头部信息header将爬虫伪装成浏览器\n",
    "* 如何获取你的浏览器的头部信息？\n",
    "    - Chrome Developer Tools (F12 or Ctrl+Shift+I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Demo: set user-agent in header\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "\n",
    "try:\n",
    "    # urllib.request.urlopen(\"http://blog.csdn.net/cqcre\") # if not setting user-agent in hearder, will throw a 403 error\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'}\n",
    "    request = urllib.request.Request(url='http://blog.csdn.net/cqcre', headers=headers)\n",
    "    response = urllib.request.urlopen(request)\n",
    "    # html = response.read()\n",
    "    # print(html.decode('utf-8'))\n",
    "except urllib.error.HTTPError as e:\n",
    "    print(e)\n",
    "except urllib.error.URLError as e:\n",
    "    print(e)\n",
    "else:\n",
    "    print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Contents\n",
    "===\n",
    "* Introduction\n",
    "    - What is web scraping? and Why?\n",
    "    - Robots.txt：爬虫/机器人协议\n",
    "* Crawling Webpage\n",
    "    - urllib in standard library\n",
    "    - Handling encoding and exception issues\n",
    "    - **requests: HTTP for Humans**\n",
    "* Parsing Webpage\n",
    "    - HTML Basics\n",
    "    - BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Requests: HTTP for Humans\n",
    "===\n",
    "\n",
    "* Requests: an elegent and simple HTTP library for Python\n",
    "    - help you handle tricky issues, e.g., string encoding, etc.\n",
    "    - **推荐使用Requests库,而不是Python标准库中的urllib包**\n",
    "* Quick tutorial: http://docs.python-requests.org/en/master/user/quickstart/\n",
    "* Installation: pip install requests (如果出现权限错误，可以尝试以管理员身份运行anaconda prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## test requests to see if it is installed correctly\n",
    "import requests\n",
    "\n",
    "r = requests.get(\"http://www.douban.com\")\n",
    "print(r.status_code)\n",
    "print(r.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Requests: 7个主要方法\n",
    "---\n",
    "* requests.request()：构造一个请求，支撑以下各方法的基础方法\n",
    "* **requests.get()**：获取HTML网页的主要方法，对应于HTTP的GET\n",
    "* requests.head()：获取HTML网页头信息的方法，对应于HTTP的HEAD\n",
    "* requests.post()：向HTML网页提交POST请求的方法，对应于HTTP的POST\n",
    "* requests.put()：向HTML网页提交PUT请求的方法，对应于HTTP的PUT\n",
    "* requests.patch()：向HTML网页提交局部修改请求，对应于HTTP的PATCH\n",
    "* requests.delete()：向HTML页面提交删除请求，对应于HTTP的DELETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Requests的get()方法\n",
    "---\n",
    "\n",
    "* r = requests.get(url)\n",
    "    - r -> 返回一个包含服务器资源的Response对象，即爬虫返回的内容\n",
    "    - get()方法构造一个向服务器请求资源的Request对象"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* requests.get(url, params=None, **kwargs)\n",
    "    - url: 拟获取页面的url地址\n",
    "    - params: url中的额外参数\n",
    "    - **kwargs: 控制访问的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "r = requests.get(\"http://www.douban.com\",timeout=0.001) # 超时参数\n",
    "print(r.status_code)\n",
    "print(type(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Response对象的常用属性\n",
    "---\n",
    "\n",
    "* r.status_code: HTTP请求的返回状态(200:成功,其他代码:失败)\n",
    "* r.text: 返回的url页面内容\n",
    "* r.encoding: 从HTTP header中推测的字符编码方式\n",
    "* r.apparent_encoding: 从文本内容中分析出的备选字符编码方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Demo: Response对象的属性和字符编码处理\n",
    "import requests\n",
    "r = requests.get(\"http://www.sjtu.edu.cn\")\n",
    "print(r.status_code)\n",
    "print(r.encoding)\n",
    "print(r.apparent_encoding)\n",
    "print(r.text)\n",
    "# r.encoding = r.apparent_encoding\n",
    "# print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Requests的异常处理\n",
    "---\n",
    "* Requests库的异常：\n",
    "    - requests.ConnectionError: 网络连接错误异常\n",
    "    - requests.HTTPError: HTTP错误异常\n",
    "    - requests.URLRequired: URL缺失异常\n",
    "    - requests.TooManyRedirection: 重定向异常\n",
    "    - requests.ConnectTimeout: 连接远程服务器超时异常\n",
    "    - requests.Timeout: URL请求超时异常\n",
    "* Requests可以自行捕获异常，如需抛出，则可使用:\n",
    "    - r.raise_for_status(): HTTP状态码如果不是200，则抛出异常requests.HTTPError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Demo: requests handles exceptions\n",
    "import requests\n",
    "r = requests.get(\"http://www.sjtu.edu.cn/1234\")\n",
    "print(r.status_code)\n",
    "# r.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "实例：抓取京东商品页面\n",
    "---\n",
    "* 使用Requests库抓取此链接中的商品页面：https://item.jd.com/497227.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"https://item.jd.com/497227.html\" #空气净化器\n",
    "r = requests.get(url, timeout=30)\n",
    "r.raise_for_status() # throw HTTPError if the status code is not 200\n",
    "r.encoding = r.apparent_encoding # handling encoding issue\n",
    "print(r.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "实例: 抓取亚马逊中国商品页面\n",
    "---\n",
    "* 使用Requests库抓取此链接中的商品页面：https://www.amazon.cn/dp/B005GNM3SS/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"https://www.amazon.cn/dp/B005GNM3SS/\" #空气净化器\n",
    "# url = \"https://www.amazon.cn/gp/product/B01ARKEV1G\" # 机器学习西瓜书\n",
    "r = requests.get(url, timeout=30)\n",
    "print(r.status_code)\n",
    "r.encoding = r.apparent_encoding # handling encoding issue\n",
    "print(r.text)\n",
    "print(r.request.headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Demo: set user-agnet using \"headers\"\n",
    "import requests\n",
    "url = \"https://www.amazon.cn/dp/B005GNM3SS/\" #空气净化器\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'}\n",
    "# headers = {'User-Agent':'Mozilla/5.0'}\n",
    "r = requests.get(url, timeout=30, headers=headers) # add header information for user-agent\n",
    "print(r.status_code)\n",
    "r.encoding = r.apparent_encoding # handling encoding issue\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "实例: 百度搜索关键词提交\n",
    "---\n",
    "* 百度的关键词接口：http://www.baidu.com/s?wd=keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Demo: set keywords to submit using \"params\"\n",
    "import requests\n",
    "\n",
    "keywords={'wd':'python scrape'}\n",
    "r = requests.get(\"http://www.baidu.com/s\",params=keywords)\n",
    "print(r.status_code)\n",
    "print(r.request.url)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Exercise:\n",
    "---\n",
    "* 使用requests库抓取你最喜欢的电影的豆瓣页面, e.g., https://movie.douban.com/subject/1298250/\n",
    "* 抓取该豆瓣电影页面后打印如下信息：\n",
    "    - 抓取该页面的HTTP状态返回码\n",
    "    - 该页面的编码\n",
    "    - 该页面的前2000个字符"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Requests抓取网页的通用代码\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Requests抓取网页的通用代码: 加入异常捕获，超时设定，编码设定，浏览器伪装\n",
    "import requests\n",
    "\n",
    "# define get_html() function\n",
    "def get_html(url):\n",
    "    try:\n",
    "        r = requests.get(url, headers={'User-Agent':'Mozilla/5.0'}, timeout=30)\n",
    "        r.raise_for_status() # throw HTTPError if the status code is not 200\n",
    "        r.encoding = r.apparent_encoding # handling encoding issue\n",
    "        return r.text\n",
    "    except:\n",
    "        return \"Error: something is Wrong!\"\n",
    "\n",
    "# call get_html() function\n",
    "url_bad = \"www.baidu.com\"\n",
    "print(get_html(url_bad))\n",
    "print()\n",
    "url = \"http://www.baidu.com\"\n",
    "print(get_html(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Contents\n",
    "===\n",
    "* Introduction\n",
    "    - What is web scraping? and Why?\n",
    "    - Robots.txt：爬虫/机器人协议\n",
    "* Crawling Webpage\n",
    "    - urllib in standard library\n",
    "    - Handling encoding and exception issues\n",
    "    - requests: HTTP for Humans\n",
    "* Parsing Webpage\n",
    "    - **HTML Basics**\n",
    "    - BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "HTML Basics\n",
    "===\n",
    "\n",
    "* 超文本标记语言（HyperText Markup Language）\n",
    "\n",
    "* 用于写网页的一种语言\n",
    "\n",
    "* HTML 标签（tags）\n",
    "    - 尖括号<>\n",
    "    - 通常成对出现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This is an example for a minimal webpage defined in HTML tags. The root tag is `<html>` and then you have the `<head>` tag. This part of the page typically includes the title of the page and might also have other meta information like the author or keywords that are important for search engines. The `<body>` tag marks the actual content of the page. You can play around with the `<h2>` tag trying different header levels. They range from 1 to 6. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "一些常见的标签\n",
    "---\n",
    "\n",
    "* heading\n",
    "    - `<h1></h1> ... <h6></h6>`\n",
    "* paragraph\n",
    "    - `<p></p>` \n",
    "* line break\n",
    "    - `<br>` \n",
    "* link with attribute\n",
    "    - `<a href=\"http://www.example.com/\">An example link</a>`\n",
    "* More details about tags can be found here:\n",
    "    - https://www.w3schools.com/tags/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "htmlString = \"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "    <title>This is a title</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h2> Test </h2>\n",
    "    <p>Hello world!</p>\n",
    "    <p><a href=\"http://yangbao.org\" target=\"_blank\">My Website</a></p>\n",
    "    \n",
    "  </body>\n",
    "</html>\"\"\"\n",
    "\n",
    "htmlOutput = HTML(htmlString)\n",
    "htmlOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Contents\n",
    "===\n",
    "* Introduction\n",
    "    - What is web scraping? and Why?\n",
    "    - Robots.txt：爬虫/机器人协议\n",
    "* Crawling Webpage\n",
    "    - urllib in standard library\n",
    "    - Handling encoding and exception issues\n",
    "    - requests: HTTP for Humans\n",
    "* Parsing Webpage\n",
    "    - HTML Basics\n",
    "    - **BeautifulSoup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "HTML页面的解析\n",
    "===\n",
    "* 大部分常用的浏览器都可以将HTML格式的文档解析成DOM（Document Object Model）结构：https://www.w3.org/DOM/\n",
    "\n",
    "![Html Dom Tree](images/HTMLDOMTree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "HTML页面的解析\n",
    "=================\n",
    "* 我们需要抓取的文本信息，通常只是DOM中HTML元素的一部分\n",
    "\n",
    "![HTML Tree](images/treeStructure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "BeautifulSoup: Parsing Webpages\n",
    "===\n",
    "\n",
    "* BeautifulSoup: a powerful library for parsing webpages\n",
    "    - 还有一些其他的类库，如lxml（事实上，BeautifulSoup支持使用lxml解析器）\n",
    "* 另外，我们可以经常使用浏览器来帮助我们理解HTML页面的结构\n",
    "    - ** 'Ctrl-Shift I' in Chrome, or 右击 -> view page source**\n",
    "* Quick tutorial: https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/\n",
    "* Installation:\n",
    "    - pip install beautifulsoup4 (如果出现权限错误，可以尝试以管理员身份运行anaconda prompt)\n",
    "    - pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "htmlString = \"<!DOCTYPE html><html><head><title>This is a title</title></head><body><h2>Test</h2><p>Hello world!</p></body></html>\"\n",
    "\n",
    "htmlOutput = HTML(htmlString)\n",
    "htmlOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## test BeautifulSoup\n",
    "\n",
    "htmlString = \"<!DOCTYPE html><html><head><title>This is a title</title></head><body><h2>Test</h2><p>Hello world!</p></body></html>\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(htmlString, \"html.parser\")\n",
    "print(htmlString)\n",
    "print()\n",
    "print(soup.prettify()) # 友好的输出：prettify()方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "BeautifulSoup库是用来解析、遍历、维护“标签树”的功能库\n",
    "---\n",
    "![BeautifulSoup](images/bs4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "BeautifulSoup类的基本元素\n",
    "---\n",
    "![BeautifulSoup_elements](images/bs4_elements.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "BeautifulSoup支持的解析器\n",
    "---\n",
    "![BeautifulSoup_parser](images/bs4_parser.png)\n",
    "* More details at: https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Use BeautifulSoup for parsing HTML\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = \"http://www.crummy.com/software/BeautifulSoup\"\n",
    "r = requests.get(url)\n",
    "\n",
    "## get BeautifulSoup object\n",
    "soup = BeautifulSoup(r.text,\"html.parser\")\n",
    "print(type(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## compare these print statements\n",
    "# print(soup)\n",
    "# print(soup.prettify())\n",
    "# print(soup.get_text()) # print text by removing tags\n",
    "\n",
    "## show how to find all a tags\n",
    "# soup.find_all('a')\n",
    "\n",
    "## ***Why does this not work? ***\n",
    "# soup.find_all('Soup')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The last command only returns an empty list, because `Soup` is not an HTML tag. It is just a string that occours in the webpage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Examples of using BeautifulSoup\n",
    "---\n",
    "More examples can be found at: https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## get attribute value from an element:\n",
    "## find tag: this only returns the first occurrence, not all tags in the string\n",
    "first_tag = soup.find('a')\n",
    "print(first_tag)\n",
    "\n",
    "## get attribute `href`\n",
    "print(first_tag.get('href'))\n",
    "print(first_tag['href'])\n",
    "\n",
    "## get text\n",
    "print(first_tag.string)\n",
    "print(first_tag.text)\n",
    "print(first_tag.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## get all links in the page\n",
    "link_list = [link.get('href') for link in soup.find_all('a')]\n",
    "print(link_list)\n",
    "\n",
    "## filter all external links\n",
    "## create an empty list to collect the valid links\n",
    "external_links = []\n",
    "\n",
    "## write a loop to filter the links\n",
    "## if it starts with 'http' we are happy\n",
    "# for l in link_list:\n",
    "#     if l[:4] == 'http':\n",
    "#         external_links.append(l)\n",
    "        \n",
    "## This throws an error! It says something about 'NoneType'\n",
    "## lets investigate. Have a close look at the link_list:\n",
    "# link_list\n",
    "\n",
    "## Seems that there are None elements! Let's verify\n",
    "# print([link for link in link_list if link is None])\n",
    "\n",
    "## So there are two elements in the list that are None!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's filter those objects out in the for loop\n",
    "external_links = []\n",
    "\n",
    "## get all links in the page\n",
    "link_list = [link.get('href') for link in soup.find_all('a')]\n",
    "\n",
    "# write a loop to filter the links\n",
    "# if it is not None and starts with 'http' we are happy\n",
    "for link in link_list:\n",
    "    if link is not None and link[:4] == 'http': # Note: lazy evaluation\n",
    "        external_links.append(link)\n",
    "        \n",
    "external_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Note: The above `if` condition works because of lazy evaluation in Python. The `and` statement becomes `False` if the first part is `False`, so there is no need to ever evaluate the second part. Thus a `None` entry in the list gets never asked about its first four characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# a more pythonic solution: use list comprehension\n",
    "link_list = [link.get('href') for link in soup.find_all('a')]\n",
    "[link for link in link_list if l is not None and l.startswith('http')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In-Class Practice:\n",
    "===\n",
    "\n",
    "* 抓取交大饮水思源BBS笑话版的所有主题帖列表（标题和链接）: https://bbs.sjtu.edu.cn/bbsdoc,board,joke.html\n",
    "* 提示1：找出url的规律\n",
    "    - 笑话版第1页url: https://bbs.sjtu.edu.cn/bbsdoc,board,joke,page,0.html\n",
    "    - 笑话版第2页url: https://bbs.sjtu.edu.cn/bbsdoc,board,joke,page,1.html\n",
    "    - 笑话版最后一页url?\n",
    "    - 每页中笑话帖子的url格式：https://bbs.sjtu.edu.cn/bbscon,board,joke,file,M.990192870.A.html\n",
    "* 提示2\n",
    "    - find_all('a')可以找出所有的链接\n",
    "    - link['href'] or link.get('href'), link.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Recap\n",
    "===\n",
    "* Introduction\n",
    "    - What is web scraping? and Why?\n",
    "    - Robots.txt：爬虫/机器人协议\n",
    "* Crawling Webpage\n",
    "    - urllib in standard library\n",
    "    - Handling encoding and exception issues\n",
    "    - requests: HTTP for Humans\n",
    "* Parsing Webpage\n",
    "    - HTML Basics\n",
    "    - BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Further Readings (Optional)\n",
    "===\n",
    "\n",
    "* Python字符串和编码: http://t.cn/R2yTUMm\n",
    "* Documentation:\n",
    "    - Requests: http://docs.python-requests.org/en/master/user/quickstart/\n",
    "    - BeautifulSoup: https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/\n",
    "    \n",
    "* Book:\n",
    "    - Ryan Mitchell, 2015. Web Scraping with Python: Collecting Data from the Modern Web. O'Reilly Media, 1st Edition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Acknowledgement\n",
    "===\n",
    "* These slides are an Jupyter notebook: http://jupyter.org/\n",
    "* Slides presented with 'live reveal': https://github.com/damianavila/RISE"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}